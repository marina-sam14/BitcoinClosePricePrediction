{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526b5172",
   "metadata": {},
   "source": [
    "## This notebook contains the Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc50e47c",
   "metadata": {},
   "source": [
    "##### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f562d1aa-4d2d-4337-9a31-3fc175480dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Visualize training history\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e99e7",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d55b59-683f-48ba-bfe6-a523afc69235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (hidden1): Linear(in_features=50, out_features=25, bias=True)\n",
      "  (hidden2): Linear(in_features=25, out_features=25, bias=True)\n",
      "  (out): Linear(in_features=25, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Choose CPU or Cuda device (If capable GPU present).\n",
    "device = 'cpu'\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(50, 25) # hidden layer\n",
    "        self.hidden2 = torch.nn.Linear(25, 25) # hidden layer\n",
    "        self.out = torch.nn.Linear(25, 1)      # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = F.relu(self.hidden1(x)) # activation function for first hidden layer\n",
    "        z = F.relu(self.hidden2(z)) # activation function for second hidden layer\n",
    "        z = self.out(z)        # linear output\n",
    "        return z\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac28bf",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d86b486-4a3f-4565-b7af-b500f1637e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1827, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-21</td>\n",
       "      <td>800.643982</td>\n",
       "      <td>834.281006</td>\n",
       "      <td>799.405029</td>\n",
       "      <td>834.281006</td>\n",
       "      <td>834.281006</td>\n",
       "      <td>155576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-22</td>\n",
       "      <td>834.179993</td>\n",
       "      <td>875.781982</td>\n",
       "      <td>834.148987</td>\n",
       "      <td>864.539978</td>\n",
       "      <td>864.539978</td>\n",
       "      <td>200027008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>864.888000</td>\n",
       "      <td>925.117004</td>\n",
       "      <td>864.677002</td>\n",
       "      <td>921.984009</td>\n",
       "      <td>921.984009</td>\n",
       "      <td>275564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-24</td>\n",
       "      <td>922.179993</td>\n",
       "      <td>923.479004</td>\n",
       "      <td>886.335022</td>\n",
       "      <td>898.822021</td>\n",
       "      <td>898.822021</td>\n",
       "      <td>137727008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-25</td>\n",
       "      <td>899.651978</td>\n",
       "      <td>899.651978</td>\n",
       "      <td>862.424011</td>\n",
       "      <td>896.182983</td>\n",
       "      <td>896.182983</td>\n",
       "      <td>143664992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  2016-12-21  800.643982  834.281006  799.405029  834.281006  834.281006   \n",
       "1  2016-12-22  834.179993  875.781982  834.148987  864.539978  864.539978   \n",
       "2  2016-12-23  864.888000  925.117004  864.677002  921.984009  921.984009   \n",
       "3  2016-12-24  922.179993  923.479004  886.335022  898.822021  898.822021   \n",
       "4  2016-12-25  899.651978  899.651978  862.424011  896.182983  896.182983   \n",
       "\n",
       "      Volume  \n",
       "0  155576000  \n",
       "1  200027008  \n",
       "2  275564000  \n",
       "3  137727008  \n",
       "4  143664992  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Data/BTC-USD.csv\")\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a020b",
   "metadata": {},
   "source": [
    "#### Scaling Dataset \n",
    "We scaled our dataset using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac31c88b-3673-4192-ae70-ef7299be1369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.000846\n",
       "1       0.001299\n",
       "2       0.002159\n",
       "3       0.001813\n",
       "4       0.001773\n",
       "          ...   \n",
       "1822    0.680117\n",
       "1823    0.689799\n",
       "1824    0.687676\n",
       "1825    0.690270\n",
       "1826    0.715508\n",
       "Name: Close, Length: 1827, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data = dataset[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "scaler = MinMaxScaler(copy=False)\n",
    "scaled_data[['Open', 'High', 'Low', 'Close', 'Volume']] = scaler.fit_transform(scaled_data[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "X = scaled_data['Close']\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f0330",
   "metadata": {},
   "source": [
    "In this moment, we had to create a TimeSeries function in order to create a scaling window. This windows helped us to create the batches for our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "699fd896-d731-473e-8b07-4f6f1f8a90bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Data.Dataset):   \n",
    "    def __init__(self, data, window):\n",
    "        self.data = torch.Tensor(data.values)\n",
    "        self.window = window\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index:index+self.window], self.data[index+self.window])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.__len__() - (self.window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92491e85",
   "metadata": {},
   "source": [
    "#### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba625c1-257a-42f9-ab92-179d0386d28b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "train_size = floor(X.size*split_ratio)\n",
    "test_size = floor(X.size*(1-split_ratio))\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X[:train_size], 50)\n",
    "test_dataset = TimeSeriesDataset(X[train_size:], 50)\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.MSELoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e19245",
   "metadata": {},
   "source": [
    "#### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc367498-5bd2-4327-97a6-a604c5fa3c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model, loss_fn, optimizer):\n",
    "    size = len(dataset)\n",
    "    tlosses = []\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataset):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X).squeeze(-1)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        tlosses.append(loss.item())\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch\n",
    "            print(f\"loss: {loss:>7f}  Batch: [{current:>5d}/{size:>5d}]\")\n",
    "    return tlosses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e541084",
   "metadata": {},
   "source": [
    "#### Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f5559e1-849f-4fe6-acd1-a7b93327c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset, model, loss_fn):\n",
    "    size = len(dataset)\n",
    "    vlosses = []\n",
    "    num_batches = len(dataset) - 50\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataset:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X).squeeze(-1)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            vlosses.append(test_loss)\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "    print(f\"Avg Accuracy:  {1-test_loss:>8f} \\n\")\n",
    "    return vlosses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc77f8",
   "metadata": {},
   "source": [
    "#### Training the model for 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08df3943-67c7-48ca-9fdb-e4e1e9df9f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.006086  Batch: [    0/ 1411]\n",
      "loss: 0.000012  Batch: [  100/ 1411]\n",
      "loss: 0.000040  Batch: [  200/ 1411]\n",
      "loss: 0.003868  Batch: [  300/ 1411]\n",
      "loss: 0.000241  Batch: [  400/ 1411]\n",
      "loss: 0.000115  Batch: [  500/ 1411]\n",
      "loss: 0.000000  Batch: [  600/ 1411]\n",
      "loss: 0.000035  Batch: [  700/ 1411]\n",
      "loss: 0.000031  Batch: [  800/ 1411]\n",
      "loss: 0.000299  Batch: [  900/ 1411]\n",
      "loss: 0.000112  Batch: [ 1000/ 1411]\n",
      "loss: 0.000119  Batch: [ 1100/ 1411]\n",
      "loss: 0.000024  Batch: [ 1200/ 1411]\n",
      "loss: 0.000032  Batch: [ 1300/ 1411]\n",
      "loss: 0.000017  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.047710 \n",
      "\n",
      "Avg Accuracy:  0.952290 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.005360  Batch: [    0/ 1411]\n",
      "loss: 0.000013  Batch: [  100/ 1411]\n",
      "loss: 0.000033  Batch: [  200/ 1411]\n",
      "loss: 0.003679  Batch: [  300/ 1411]\n",
      "loss: 0.000229  Batch: [  400/ 1411]\n",
      "loss: 0.000113  Batch: [  500/ 1411]\n",
      "loss: 0.000000  Batch: [  600/ 1411]\n",
      "loss: 0.000040  Batch: [  700/ 1411]\n",
      "loss: 0.000027  Batch: [  800/ 1411]\n",
      "loss: 0.000280  Batch: [  900/ 1411]\n",
      "loss: 0.000098  Batch: [ 1000/ 1411]\n",
      "loss: 0.000122  Batch: [ 1100/ 1411]\n",
      "loss: 0.000029  Batch: [ 1200/ 1411]\n",
      "loss: 0.000031  Batch: [ 1300/ 1411]\n",
      "loss: 0.000026  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.043844 \n",
      "\n",
      "Avg Accuracy:  0.956156 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.004787  Batch: [    0/ 1411]\n",
      "loss: 0.000014  Batch: [  100/ 1411]\n",
      "loss: 0.000027  Batch: [  200/ 1411]\n",
      "loss: 0.003543  Batch: [  300/ 1411]\n",
      "loss: 0.000216  Batch: [  400/ 1411]\n",
      "loss: 0.000119  Batch: [  500/ 1411]\n",
      "loss: 0.000000  Batch: [  600/ 1411]\n",
      "loss: 0.000046  Batch: [  700/ 1411]\n",
      "loss: 0.000024  Batch: [  800/ 1411]\n",
      "loss: 0.000261  Batch: [  900/ 1411]\n",
      "loss: 0.000094  Batch: [ 1000/ 1411]\n",
      "loss: 0.000132  Batch: [ 1100/ 1411]\n",
      "loss: 0.000027  Batch: [ 1200/ 1411]\n",
      "loss: 0.000028  Batch: [ 1300/ 1411]\n",
      "loss: 0.000035  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.040352 \n",
      "\n",
      "Avg Accuracy:  0.959648 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.004271  Batch: [    0/ 1411]\n",
      "loss: 0.000014  Batch: [  100/ 1411]\n",
      "loss: 0.000023  Batch: [  200/ 1411]\n",
      "loss: 0.003442  Batch: [  300/ 1411]\n",
      "loss: 0.000204  Batch: [  400/ 1411]\n",
      "loss: 0.000125  Batch: [  500/ 1411]\n",
      "loss: 0.000000  Batch: [  600/ 1411]\n",
      "loss: 0.000051  Batch: [  700/ 1411]\n",
      "loss: 0.000021  Batch: [  800/ 1411]\n",
      "loss: 0.000257  Batch: [  900/ 1411]\n",
      "loss: 0.000097  Batch: [ 1000/ 1411]\n",
      "loss: 0.000133  Batch: [ 1100/ 1411]\n",
      "loss: 0.000026  Batch: [ 1200/ 1411]\n",
      "loss: 0.000025  Batch: [ 1300/ 1411]\n",
      "loss: 0.000046  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.037183 \n",
      "\n",
      "Avg Accuracy:  0.962817 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.003719  Batch: [    0/ 1411]\n",
      "loss: 0.000014  Batch: [  100/ 1411]\n",
      "loss: 0.000019  Batch: [  200/ 1411]\n",
      "loss: 0.003337  Batch: [  300/ 1411]\n",
      "loss: 0.000194  Batch: [  400/ 1411]\n",
      "loss: 0.000126  Batch: [  500/ 1411]\n",
      "loss: 0.000000  Batch: [  600/ 1411]\n",
      "loss: 0.000057  Batch: [  700/ 1411]\n",
      "loss: 0.000018  Batch: [  800/ 1411]\n",
      "loss: 0.000255  Batch: [  900/ 1411]\n",
      "loss: 0.000097  Batch: [ 1000/ 1411]\n",
      "loss: 0.000129  Batch: [ 1100/ 1411]\n",
      "loss: 0.000027  Batch: [ 1200/ 1411]\n",
      "loss: 0.000022  Batch: [ 1300/ 1411]\n",
      "loss: 0.000057  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.034436 \n",
      "\n",
      "Avg Accuracy:  0.965564 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.003240  Batch: [    0/ 1411]\n",
      "loss: 0.000015  Batch: [  100/ 1411]\n",
      "loss: 0.000016  Batch: [  200/ 1411]\n",
      "loss: 0.003239  Batch: [  300/ 1411]\n",
      "loss: 0.000186  Batch: [  400/ 1411]\n",
      "loss: 0.000126  Batch: [  500/ 1411]\n",
      "loss: 0.000000  Batch: [  600/ 1411]\n",
      "loss: 0.000061  Batch: [  700/ 1411]\n",
      "loss: 0.000015  Batch: [  800/ 1411]\n",
      "loss: 0.000254  Batch: [  900/ 1411]\n",
      "loss: 0.000097  Batch: [ 1000/ 1411]\n",
      "loss: 0.000124  Batch: [ 1100/ 1411]\n",
      "loss: 0.000028  Batch: [ 1200/ 1411]\n",
      "loss: 0.000020  Batch: [ 1300/ 1411]\n",
      "loss: 0.000067  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.032061 \n",
      "\n",
      "Avg Accuracy:  0.967939 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.002823  Batch: [    0/ 1411]\n",
      "loss: 0.000015  Batch: [  100/ 1411]\n",
      "loss: 0.000013  Batch: [  200/ 1411]\n",
      "loss: 0.003145  Batch: [  300/ 1411]\n",
      "loss: 0.000180  Batch: [  400/ 1411]\n",
      "loss: 0.000126  Batch: [  500/ 1411]\n",
      "loss: 0.000000  Batch: [  600/ 1411]\n",
      "loss: 0.000065  Batch: [  700/ 1411]\n",
      "loss: 0.000013  Batch: [  800/ 1411]\n",
      "loss: 0.000252  Batch: [  900/ 1411]\n",
      "loss: 0.000097  Batch: [ 1000/ 1411]\n",
      "loss: 0.000119  Batch: [ 1100/ 1411]\n",
      "loss: 0.000030  Batch: [ 1200/ 1411]\n",
      "loss: 0.000019  Batch: [ 1300/ 1411]\n",
      "loss: 0.000076  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.030036 \n",
      "\n",
      "Avg Accuracy:  0.969964 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.002473  Batch: [    0/ 1411]\n",
      "loss: 0.000015  Batch: [  100/ 1411]\n",
      "loss: 0.000011  Batch: [  200/ 1411]\n",
      "loss: 0.003056  Batch: [  300/ 1411]\n",
      "loss: 0.000174  Batch: [  400/ 1411]\n",
      "loss: 0.000126  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000069  Batch: [  700/ 1411]\n",
      "loss: 0.000012  Batch: [  800/ 1411]\n",
      "loss: 0.000249  Batch: [  900/ 1411]\n",
      "loss: 0.000096  Batch: [ 1000/ 1411]\n",
      "loss: 0.000113  Batch: [ 1100/ 1411]\n",
      "loss: 0.000032  Batch: [ 1200/ 1411]\n",
      "loss: 0.000018  Batch: [ 1300/ 1411]\n",
      "loss: 0.000086  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.028277 \n",
      "\n",
      "Avg Accuracy:  0.971723 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.002170  Batch: [    0/ 1411]\n",
      "loss: 0.000013  Batch: [  100/ 1411]\n",
      "loss: 0.000009  Batch: [  200/ 1411]\n",
      "loss: 0.002971  Batch: [  300/ 1411]\n",
      "loss: 0.000170  Batch: [  400/ 1411]\n",
      "loss: 0.000127  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000072  Batch: [  700/ 1411]\n",
      "loss: 0.000010  Batch: [  800/ 1411]\n",
      "loss: 0.000245  Batch: [  900/ 1411]\n",
      "loss: 0.000094  Batch: [ 1000/ 1411]\n",
      "loss: 0.000108  Batch: [ 1100/ 1411]\n",
      "loss: 0.000035  Batch: [ 1200/ 1411]\n",
      "loss: 0.000016  Batch: [ 1300/ 1411]\n",
      "loss: 0.000094  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.026758 \n",
      "\n",
      "Avg Accuracy:  0.973242 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.001917  Batch: [    0/ 1411]\n",
      "loss: 0.000013  Batch: [  100/ 1411]\n",
      "loss: 0.000007  Batch: [  200/ 1411]\n",
      "loss: 0.002889  Batch: [  300/ 1411]\n",
      "loss: 0.000167  Batch: [  400/ 1411]\n",
      "loss: 0.000126  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000075  Batch: [  700/ 1411]\n",
      "loss: 0.000009  Batch: [  800/ 1411]\n",
      "loss: 0.000242  Batch: [  900/ 1411]\n",
      "loss: 0.000093  Batch: [ 1000/ 1411]\n",
      "loss: 0.000103  Batch: [ 1100/ 1411]\n",
      "loss: 0.000037  Batch: [ 1200/ 1411]\n",
      "loss: 0.000015  Batch: [ 1300/ 1411]\n",
      "loss: 0.000102  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.025442 \n",
      "\n",
      "Avg Accuracy:  0.974558 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.001691  Batch: [    0/ 1411]\n",
      "loss: 0.000012  Batch: [  100/ 1411]\n",
      "loss: 0.000006  Batch: [  200/ 1411]\n",
      "loss: 0.002814  Batch: [  300/ 1411]\n",
      "loss: 0.000167  Batch: [  400/ 1411]\n",
      "loss: 0.000122  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000077  Batch: [  700/ 1411]\n",
      "loss: 0.000008  Batch: [  800/ 1411]\n",
      "loss: 0.000234  Batch: [  900/ 1411]\n",
      "loss: 0.000091  Batch: [ 1000/ 1411]\n",
      "loss: 0.000098  Batch: [ 1100/ 1411]\n",
      "loss: 0.000039  Batch: [ 1200/ 1411]\n",
      "loss: 0.000016  Batch: [ 1300/ 1411]\n",
      "loss: 0.000109  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.024305 \n",
      "\n",
      "Avg Accuracy:  0.975695 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.001505  Batch: [    0/ 1411]\n",
      "loss: 0.000012  Batch: [  100/ 1411]\n",
      "loss: 0.000005  Batch: [  200/ 1411]\n",
      "loss: 0.002743  Batch: [  300/ 1411]\n",
      "loss: 0.000168  Batch: [  400/ 1411]\n",
      "loss: 0.000119  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000080  Batch: [  700/ 1411]\n",
      "loss: 0.000007  Batch: [  800/ 1411]\n",
      "loss: 0.000223  Batch: [  900/ 1411]\n",
      "loss: 0.000089  Batch: [ 1000/ 1411]\n",
      "loss: 0.000094  Batch: [ 1100/ 1411]\n",
      "loss: 0.000041  Batch: [ 1200/ 1411]\n",
      "loss: 0.000017  Batch: [ 1300/ 1411]\n",
      "loss: 0.000116  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.023325 \n",
      "\n",
      "Avg Accuracy:  0.976675 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.001349  Batch: [    0/ 1411]\n",
      "loss: 0.000012  Batch: [  100/ 1411]\n",
      "loss: 0.000004  Batch: [  200/ 1411]\n",
      "loss: 0.002678  Batch: [  300/ 1411]\n",
      "loss: 0.000173  Batch: [  400/ 1411]\n",
      "loss: 0.000116  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000082  Batch: [  700/ 1411]\n",
      "loss: 0.000006  Batch: [  800/ 1411]\n",
      "loss: 0.000212  Batch: [  900/ 1411]\n",
      "loss: 0.000085  Batch: [ 1000/ 1411]\n",
      "loss: 0.000090  Batch: [ 1100/ 1411]\n",
      "loss: 0.000043  Batch: [ 1200/ 1411]\n",
      "loss: 0.000018  Batch: [ 1300/ 1411]\n",
      "loss: 0.000121  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.022480 \n",
      "\n",
      "Avg Accuracy:  0.977520 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.001217  Batch: [    0/ 1411]\n",
      "loss: 0.000012  Batch: [  100/ 1411]\n",
      "loss: 0.000003  Batch: [  200/ 1411]\n",
      "loss: 0.002617  Batch: [  300/ 1411]\n",
      "loss: 0.000181  Batch: [  400/ 1411]\n",
      "loss: 0.000114  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000084  Batch: [  700/ 1411]\n",
      "loss: 0.000005  Batch: [  800/ 1411]\n",
      "loss: 0.000201  Batch: [  900/ 1411]\n",
      "loss: 0.000082  Batch: [ 1000/ 1411]\n",
      "loss: 0.000086  Batch: [ 1100/ 1411]\n",
      "loss: 0.000045  Batch: [ 1200/ 1411]\n",
      "loss: 0.000019  Batch: [ 1300/ 1411]\n",
      "loss: 0.000126  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.021729 \n",
      "\n",
      "Avg Accuracy:  0.978271 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.001097  Batch: [    0/ 1411]\n",
      "loss: 0.000012  Batch: [  100/ 1411]\n",
      "loss: 0.000002  Batch: [  200/ 1411]\n",
      "loss: 0.002561  Batch: [  300/ 1411]\n",
      "loss: 0.000184  Batch: [  400/ 1411]\n",
      "loss: 0.000111  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000085  Batch: [  700/ 1411]\n",
      "loss: 0.000004  Batch: [  800/ 1411]\n",
      "loss: 0.000193  Batch: [  900/ 1411]\n",
      "loss: 0.000078  Batch: [ 1000/ 1411]\n",
      "loss: 0.000082  Batch: [ 1100/ 1411]\n",
      "loss: 0.000047  Batch: [ 1200/ 1411]\n",
      "loss: 0.000019  Batch: [ 1300/ 1411]\n",
      "loss: 0.000131  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.021060 \n",
      "\n",
      "Avg Accuracy:  0.978940 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.000994  Batch: [    0/ 1411]\n",
      "loss: 0.000012  Batch: [  100/ 1411]\n",
      "loss: 0.000002  Batch: [  200/ 1411]\n",
      "loss: 0.002510  Batch: [  300/ 1411]\n",
      "loss: 0.000183  Batch: [  400/ 1411]\n",
      "loss: 0.000109  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000087  Batch: [  700/ 1411]\n",
      "loss: 0.000004  Batch: [  800/ 1411]\n",
      "loss: 0.000184  Batch: [  900/ 1411]\n",
      "loss: 0.000074  Batch: [ 1000/ 1411]\n",
      "loss: 0.000079  Batch: [ 1100/ 1411]\n",
      "loss: 0.000049  Batch: [ 1200/ 1411]\n",
      "loss: 0.000019  Batch: [ 1300/ 1411]\n",
      "loss: 0.000135  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.020479 \n",
      "\n",
      "Avg Accuracy:  0.979521 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.000906  Batch: [    0/ 1411]\n",
      "loss: 0.000012  Batch: [  100/ 1411]\n",
      "loss: 0.000001  Batch: [  200/ 1411]\n",
      "loss: 0.002460  Batch: [  300/ 1411]\n",
      "loss: 0.000183  Batch: [  400/ 1411]\n",
      "loss: 0.000108  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000088  Batch: [  700/ 1411]\n",
      "loss: 0.000003  Batch: [  800/ 1411]\n",
      "loss: 0.000176  Batch: [  900/ 1411]\n",
      "loss: 0.000069  Batch: [ 1000/ 1411]\n",
      "loss: 0.000076  Batch: [ 1100/ 1411]\n",
      "loss: 0.000051  Batch: [ 1200/ 1411]\n",
      "loss: 0.000019  Batch: [ 1300/ 1411]\n",
      "loss: 0.000139  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.019953 \n",
      "\n",
      "Avg Accuracy:  0.980047 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.000828  Batch: [    0/ 1411]\n",
      "loss: 0.000012  Batch: [  100/ 1411]\n",
      "loss: 0.000001  Batch: [  200/ 1411]\n",
      "loss: 0.002411  Batch: [  300/ 1411]\n",
      "loss: 0.000182  Batch: [  400/ 1411]\n",
      "loss: 0.000106  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000089  Batch: [  700/ 1411]\n",
      "loss: 0.000002  Batch: [  800/ 1411]\n",
      "loss: 0.000168  Batch: [  900/ 1411]\n",
      "loss: 0.000064  Batch: [ 1000/ 1411]\n",
      "loss: 0.000073  Batch: [ 1100/ 1411]\n",
      "loss: 0.000053  Batch: [ 1200/ 1411]\n",
      "loss: 0.000019  Batch: [ 1300/ 1411]\n",
      "loss: 0.000142  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.019474 \n",
      "\n",
      "Avg Accuracy:  0.980526 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.000759  Batch: [    0/ 1411]\n",
      "loss: 0.000012  Batch: [  100/ 1411]\n",
      "loss: 0.000001  Batch: [  200/ 1411]\n",
      "loss: 0.002366  Batch: [  300/ 1411]\n",
      "loss: 0.000182  Batch: [  400/ 1411]\n",
      "loss: 0.000104  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000090  Batch: [  700/ 1411]\n",
      "loss: 0.000002  Batch: [  800/ 1411]\n",
      "loss: 0.000160  Batch: [  900/ 1411]\n",
      "loss: 0.000059  Batch: [ 1000/ 1411]\n",
      "loss: 0.000070  Batch: [ 1100/ 1411]\n",
      "loss: 0.000055  Batch: [ 1200/ 1411]\n",
      "loss: 0.000019  Batch: [ 1300/ 1411]\n",
      "loss: 0.000145  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.019023 \n",
      "\n",
      "Avg Accuracy:  0.980977 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.000694  Batch: [    0/ 1411]\n",
      "loss: 0.000011  Batch: [  100/ 1411]\n",
      "loss: 0.000000  Batch: [  200/ 1411]\n",
      "loss: 0.002321  Batch: [  300/ 1411]\n",
      "loss: 0.000182  Batch: [  400/ 1411]\n",
      "loss: 0.000103  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000091  Batch: [  700/ 1411]\n",
      "loss: 0.000002  Batch: [  800/ 1411]\n",
      "loss: 0.000153  Batch: [  900/ 1411]\n",
      "loss: 0.000054  Batch: [ 1000/ 1411]\n",
      "loss: 0.000069  Batch: [ 1100/ 1411]\n",
      "loss: 0.000057  Batch: [ 1200/ 1411]\n",
      "loss: 0.000020  Batch: [ 1300/ 1411]\n",
      "loss: 0.000148  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.018615 \n",
      "\n",
      "Avg Accuracy:  0.981385 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "tlosses = []\n",
    "vlosses = []\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    etlosses = train(train_dataset, model, loss_fn, optimizer)\n",
    "    tlosses.append(sum(etlosses)/len(etlosses))\n",
    "    evlosses = test(test_dataset, model, loss_fn)\n",
    "    vlosses.append(sum(evlosses)/len(evlosses))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46030f1b",
   "metadata": {},
   "source": [
    "plt.plot(np.linspace(1, EPOCHS, EPOCHS).astype(int), tlosses)#### Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cee7f2be-5c4e-40c9-8a35-f503011f1726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next-day price based on 50 previous ones: 36635.125\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "data = dataset['Close']\n",
    "data = torch.Tensor(data[-50:].values)\n",
    "output = model(data)\n",
    "print(f'Predicted next-day price based on 50 previous ones: {output.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cda6b3-4b9d-4d12-b5c5-51a447db1c3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plotting Losses (Training and Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34703309-3c2c-400a-a87f-6b6f29c3c4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1db7012ab20>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(1, EPOCHS, EPOCHS), tlosses)\n",
    "plt.plot(np.linspace(1, EPOCHS, EPOCHS), vlosses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
