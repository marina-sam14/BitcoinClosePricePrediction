{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f562d1aa-4d2d-4337-9a31-3fc175480dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import floor\n",
    "import matplotlib.pyplot  as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Visualize training history\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e99e7",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d55b59-683f-48ba-bfe6-a523afc69235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (hidden1): Linear(in_features=50, out_features=25, bias=True)\n",
      "  (hidden2): Linear(in_features=25, out_features=25, bias=True)\n",
      "  (out): Linear(in_features=25, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Choose CPU or Cuda device (If capable GPU present).\n",
    "device = 'cpu'\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(50, 25) # hidden layer\n",
    "        self.hidden2 = torch.nn.Linear(25, 25) # hidden layer\n",
    "        self.out = torch.nn.Linear(25, 1)      # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = F.relu(self.hidden1(x)) # activation function for first hidden layer\n",
    "        z = F.relu(self.hidden2(z)) # activation function for second hidden layer\n",
    "        z = self.out(z)        # linear output\n",
    "        return z\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac28bf",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d86b486-4a3f-4565-b7af-b500f1637e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1827, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-21</td>\n",
       "      <td>800.643982</td>\n",
       "      <td>834.281006</td>\n",
       "      <td>799.405029</td>\n",
       "      <td>834.281006</td>\n",
       "      <td>834.281006</td>\n",
       "      <td>155576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-22</td>\n",
       "      <td>834.179993</td>\n",
       "      <td>875.781982</td>\n",
       "      <td>834.148987</td>\n",
       "      <td>864.539978</td>\n",
       "      <td>864.539978</td>\n",
       "      <td>200027008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>864.888000</td>\n",
       "      <td>925.117004</td>\n",
       "      <td>864.677002</td>\n",
       "      <td>921.984009</td>\n",
       "      <td>921.984009</td>\n",
       "      <td>275564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-24</td>\n",
       "      <td>922.179993</td>\n",
       "      <td>923.479004</td>\n",
       "      <td>886.335022</td>\n",
       "      <td>898.822021</td>\n",
       "      <td>898.822021</td>\n",
       "      <td>137727008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-25</td>\n",
       "      <td>899.651978</td>\n",
       "      <td>899.651978</td>\n",
       "      <td>862.424011</td>\n",
       "      <td>896.182983</td>\n",
       "      <td>896.182983</td>\n",
       "      <td>143664992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  2016-12-21  800.643982  834.281006  799.405029  834.281006  834.281006   \n",
       "1  2016-12-22  834.179993  875.781982  834.148987  864.539978  864.539978   \n",
       "2  2016-12-23  864.888000  925.117004  864.677002  921.984009  921.984009   \n",
       "3  2016-12-24  922.179993  923.479004  886.335022  898.822021  898.822021   \n",
       "4  2016-12-25  899.651978  899.651978  862.424011  896.182983  896.182983   \n",
       "\n",
       "      Volume  \n",
       "0  155576000  \n",
       "1  200027008  \n",
       "2  275564000  \n",
       "3  137727008  \n",
       "4  143664992  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Data/BTC-USD.csv\")\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a020b",
   "metadata": {},
   "source": [
    "#### Scaling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac31c88b-3673-4192-ae70-ef7299be1369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Μαρίνα Σαμ\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:3678: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[col] = igetitem(value, i)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       0.000846\n",
       "1       0.001299\n",
       "2       0.002159\n",
       "3       0.001813\n",
       "4       0.001773\n",
       "          ...   \n",
       "1822    0.680117\n",
       "1823    0.689799\n",
       "1824    0.687676\n",
       "1825    0.690270\n",
       "1826    0.715508\n",
       "Name: Close, Length: 1827, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data = dataset[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "scaler = MinMaxScaler(copy=False)\n",
    "scaled_data[['Open', 'High', 'Low', 'Close', 'Volume']] = scaler.fit_transform(scaled_data[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "X = scaled_data['Close']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "699fd896-d731-473e-8b07-4f6f1f8a90bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Data.Dataset):   \n",
    "    def __init__(self, data, window):\n",
    "        self.data = torch.Tensor(data.values)\n",
    "        self.window = window\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index:index+self.window], self.data[index+self.window])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.__len__() - (self.window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92491e85",
   "metadata": {},
   "source": [
    "#### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba625c1-257a-42f9-ab92-179d0386d28b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "train_size = floor(X.size*split_ratio)\n",
    "test_size = floor(X.size*(1-split_ratio))\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X[:train_size], 50)\n",
    "test_dataset = TimeSeriesDataset(X[train_size:], 50)\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.MSELoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e19245",
   "metadata": {},
   "source": [
    "#### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc367498-5bd2-4327-97a6-a604c5fa3c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model, loss_fn, optimizer):\n",
    "    size = len(dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataset):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X).squeeze(-1)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch\n",
    "            print(f\"loss: {loss:>7f}  Batch: [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e541084",
   "metadata": {},
   "source": [
    "#### Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f5559e1-849f-4fe6-acd1-a7b93327c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset, model, loss_fn):\n",
    "    size = len(dataset)\n",
    "    num_batches = len(dataset) - 50\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataset:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X).squeeze(-1)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc77f8",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08df3943-67c7-48ca-9fdb-e4e1e9df9f9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.033330  Batch: [    0/ 1411]\n",
      "loss: 0.000547  Batch: [  100/ 1411]\n",
      "loss: 0.000395  Batch: [  200/ 1411]\n",
      "loss: 0.011511  Batch: [  300/ 1411]\n",
      "loss: 0.001070  Batch: [  400/ 1411]\n",
      "loss: 0.000552  Batch: [  500/ 1411]\n",
      "loss: 0.000015  Batch: [  600/ 1411]\n",
      "loss: 0.000136  Batch: [  700/ 1411]\n",
      "loss: 0.000198  Batch: [  800/ 1411]\n",
      "loss: 0.000001  Batch: [  900/ 1411]\n",
      "loss: 0.000024  Batch: [ 1000/ 1411]\n",
      "loss: 0.000548  Batch: [ 1100/ 1411]\n",
      "loss: 0.000053  Batch: [ 1200/ 1411]\n",
      "loss: 0.000292  Batch: [ 1300/ 1411]\n",
      "loss: 0.001326  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.273505 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.055735  Batch: [    0/ 1411]\n",
      "loss: 0.000039  Batch: [  100/ 1411]\n",
      "loss: 0.000305  Batch: [  200/ 1411]\n",
      "loss: 0.011182  Batch: [  300/ 1411]\n",
      "loss: 0.001021  Batch: [  400/ 1411]\n",
      "loss: 0.000535  Batch: [  500/ 1411]\n",
      "loss: 0.000014  Batch: [  600/ 1411]\n",
      "loss: 0.000128  Batch: [  700/ 1411]\n",
      "loss: 0.000191  Batch: [  800/ 1411]\n",
      "loss: 0.000003  Batch: [  900/ 1411]\n",
      "loss: 0.000027  Batch: [ 1000/ 1411]\n",
      "loss: 0.000530  Batch: [ 1100/ 1411]\n",
      "loss: 0.000048  Batch: [ 1200/ 1411]\n",
      "loss: 0.000277  Batch: [ 1300/ 1411]\n",
      "loss: 0.001215  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.258645 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.050923  Batch: [    0/ 1411]\n",
      "loss: 0.000029  Batch: [  100/ 1411]\n",
      "loss: 0.000279  Batch: [  200/ 1411]\n",
      "loss: 0.010883  Batch: [  300/ 1411]\n",
      "loss: 0.000984  Batch: [  400/ 1411]\n",
      "loss: 0.000532  Batch: [  500/ 1411]\n",
      "loss: 0.000010  Batch: [  600/ 1411]\n",
      "loss: 0.000098  Batch: [  700/ 1411]\n",
      "loss: 0.000176  Batch: [  800/ 1411]\n",
      "loss: 0.000008  Batch: [  900/ 1411]\n",
      "loss: 0.000029  Batch: [ 1000/ 1411]\n",
      "loss: 0.000520  Batch: [ 1100/ 1411]\n",
      "loss: 0.000042  Batch: [ 1200/ 1411]\n",
      "loss: 0.000262  Batch: [ 1300/ 1411]\n",
      "loss: 0.001104  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.244172 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.047598  Batch: [    0/ 1411]\n",
      "loss: 0.000024  Batch: [  100/ 1411]\n",
      "loss: 0.000267  Batch: [  200/ 1411]\n",
      "loss: 0.010525  Batch: [  300/ 1411]\n",
      "loss: 0.000936  Batch: [  400/ 1411]\n",
      "loss: 0.000519  Batch: [  500/ 1411]\n",
      "loss: 0.000008  Batch: [  600/ 1411]\n",
      "loss: 0.000082  Batch: [  700/ 1411]\n",
      "loss: 0.000168  Batch: [  800/ 1411]\n",
      "loss: 0.000015  Batch: [  900/ 1411]\n",
      "loss: 0.000032  Batch: [ 1000/ 1411]\n",
      "loss: 0.000510  Batch: [ 1100/ 1411]\n",
      "loss: 0.000038  Batch: [ 1200/ 1411]\n",
      "loss: 0.000236  Batch: [ 1300/ 1411]\n",
      "loss: 0.001003  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.229470 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.044405  Batch: [    0/ 1411]\n",
      "loss: 0.000020  Batch: [  100/ 1411]\n",
      "loss: 0.000255  Batch: [  200/ 1411]\n",
      "loss: 0.010146  Batch: [  300/ 1411]\n",
      "loss: 0.000892  Batch: [  400/ 1411]\n",
      "loss: 0.000493  Batch: [  500/ 1411]\n",
      "loss: 0.000008  Batch: [  600/ 1411]\n",
      "loss: 0.000069  Batch: [  700/ 1411]\n",
      "loss: 0.000161  Batch: [  800/ 1411]\n",
      "loss: 0.000024  Batch: [  900/ 1411]\n",
      "loss: 0.000037  Batch: [ 1000/ 1411]\n",
      "loss: 0.000491  Batch: [ 1100/ 1411]\n",
      "loss: 0.000034  Batch: [ 1200/ 1411]\n",
      "loss: 0.000228  Batch: [ 1300/ 1411]\n",
      "loss: 0.000901  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.214461 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.041523  Batch: [    0/ 1411]\n",
      "loss: 0.000015  Batch: [  100/ 1411]\n",
      "loss: 0.000242  Batch: [  200/ 1411]\n",
      "loss: 0.009740  Batch: [  300/ 1411]\n",
      "loss: 0.000839  Batch: [  400/ 1411]\n",
      "loss: 0.000467  Batch: [  500/ 1411]\n",
      "loss: 0.000006  Batch: [  600/ 1411]\n",
      "loss: 0.000059  Batch: [  700/ 1411]\n",
      "loss: 0.000154  Batch: [  800/ 1411]\n",
      "loss: 0.000036  Batch: [  900/ 1411]\n",
      "loss: 0.000042  Batch: [ 1000/ 1411]\n",
      "loss: 0.000470  Batch: [ 1100/ 1411]\n",
      "loss: 0.000028  Batch: [ 1200/ 1411]\n",
      "loss: 0.000206  Batch: [ 1300/ 1411]\n",
      "loss: 0.000743  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.198900 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.038582  Batch: [    0/ 1411]\n",
      "loss: 0.000011  Batch: [  100/ 1411]\n",
      "loss: 0.000229  Batch: [  200/ 1411]\n",
      "loss: 0.009333  Batch: [  300/ 1411]\n",
      "loss: 0.000794  Batch: [  400/ 1411]\n",
      "loss: 0.000441  Batch: [  500/ 1411]\n",
      "loss: 0.000005  Batch: [  600/ 1411]\n",
      "loss: 0.000049  Batch: [  700/ 1411]\n",
      "loss: 0.000147  Batch: [  800/ 1411]\n",
      "loss: 0.000052  Batch: [  900/ 1411]\n",
      "loss: 0.000047  Batch: [ 1000/ 1411]\n",
      "loss: 0.000445  Batch: [ 1100/ 1411]\n",
      "loss: 0.000021  Batch: [ 1200/ 1411]\n",
      "loss: 0.000185  Batch: [ 1300/ 1411]\n",
      "loss: 0.000540  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.183556 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.035244  Batch: [    0/ 1411]\n",
      "loss: 0.000007  Batch: [  100/ 1411]\n",
      "loss: 0.000215  Batch: [  200/ 1411]\n",
      "loss: 0.008973  Batch: [  300/ 1411]\n",
      "loss: 0.000721  Batch: [  400/ 1411]\n",
      "loss: 0.000409  Batch: [  500/ 1411]\n",
      "loss: 0.000004  Batch: [  600/ 1411]\n",
      "loss: 0.000039  Batch: [  700/ 1411]\n",
      "loss: 0.000139  Batch: [  800/ 1411]\n",
      "loss: 0.000070  Batch: [  900/ 1411]\n",
      "loss: 0.000052  Batch: [ 1000/ 1411]\n",
      "loss: 0.000417  Batch: [ 1100/ 1411]\n",
      "loss: 0.000015  Batch: [ 1200/ 1411]\n",
      "loss: 0.000166  Batch: [ 1300/ 1411]\n",
      "loss: 0.000391  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.168654 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.031712  Batch: [    0/ 1411]\n",
      "loss: 0.000004  Batch: [  100/ 1411]\n",
      "loss: 0.000201  Batch: [  200/ 1411]\n",
      "loss: 0.008600  Batch: [  300/ 1411]\n",
      "loss: 0.000559  Batch: [  400/ 1411]\n",
      "loss: 0.000377  Batch: [  500/ 1411]\n",
      "loss: 0.000004  Batch: [  600/ 1411]\n",
      "loss: 0.000031  Batch: [  700/ 1411]\n",
      "loss: 0.000131  Batch: [  800/ 1411]\n",
      "loss: 0.000104  Batch: [  900/ 1411]\n",
      "loss: 0.000068  Batch: [ 1000/ 1411]\n",
      "loss: 0.000389  Batch: [ 1100/ 1411]\n",
      "loss: 0.000010  Batch: [ 1200/ 1411]\n",
      "loss: 0.000118  Batch: [ 1300/ 1411]\n",
      "loss: 0.000295  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.154425 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.027750  Batch: [    0/ 1411]\n",
      "loss: 0.000001  Batch: [  100/ 1411]\n",
      "loss: 0.000187  Batch: [  200/ 1411]\n",
      "loss: 0.007674  Batch: [  300/ 1411]\n",
      "loss: 0.000491  Batch: [  400/ 1411]\n",
      "loss: 0.000299  Batch: [  500/ 1411]\n",
      "loss: 0.000003  Batch: [  600/ 1411]\n",
      "loss: 0.000023  Batch: [  700/ 1411]\n",
      "loss: 0.000123  Batch: [  800/ 1411]\n",
      "loss: 0.000158  Batch: [  900/ 1411]\n",
      "loss: 0.000088  Batch: [ 1000/ 1411]\n",
      "loss: 0.000304  Batch: [ 1100/ 1411]\n",
      "loss: 0.000006  Batch: [ 1200/ 1411]\n",
      "loss: 0.000100  Batch: [ 1300/ 1411]\n",
      "loss: 0.000219  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.140962 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.023893  Batch: [    0/ 1411]\n",
      "loss: 0.000000  Batch: [  100/ 1411]\n",
      "loss: 0.000174  Batch: [  200/ 1411]\n",
      "loss: 0.006941  Batch: [  300/ 1411]\n",
      "loss: 0.000464  Batch: [  400/ 1411]\n",
      "loss: 0.000275  Batch: [  500/ 1411]\n",
      "loss: 0.000002  Batch: [  600/ 1411]\n",
      "loss: 0.000005  Batch: [  700/ 1411]\n",
      "loss: 0.000105  Batch: [  800/ 1411]\n",
      "loss: 0.000190  Batch: [  900/ 1411]\n",
      "loss: 0.000096  Batch: [ 1000/ 1411]\n",
      "loss: 0.000278  Batch: [ 1100/ 1411]\n",
      "loss: 0.000002  Batch: [ 1200/ 1411]\n",
      "loss: 0.000090  Batch: [ 1300/ 1411]\n",
      "loss: 0.000148  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.127961 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.020732  Batch: [    0/ 1411]\n",
      "loss: 0.000000  Batch: [  100/ 1411]\n",
      "loss: 0.000140  Batch: [  200/ 1411]\n",
      "loss: 0.006517  Batch: [  300/ 1411]\n",
      "loss: 0.000428  Batch: [  400/ 1411]\n",
      "loss: 0.000251  Batch: [  500/ 1411]\n",
      "loss: 0.000002  Batch: [  600/ 1411]\n",
      "loss: 0.000001  Batch: [  700/ 1411]\n",
      "loss: 0.000096  Batch: [  800/ 1411]\n",
      "loss: 0.000217  Batch: [  900/ 1411]\n",
      "loss: 0.000105  Batch: [ 1000/ 1411]\n",
      "loss: 0.000253  Batch: [ 1100/ 1411]\n",
      "loss: 0.000001  Batch: [ 1200/ 1411]\n",
      "loss: 0.000078  Batch: [ 1300/ 1411]\n",
      "loss: 0.000094  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.115684 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.018206  Batch: [    0/ 1411]\n",
      "loss: 0.000002  Batch: [  100/ 1411]\n",
      "loss: 0.000126  Batch: [  200/ 1411]\n",
      "loss: 0.006112  Batch: [  300/ 1411]\n",
      "loss: 0.000389  Batch: [  400/ 1411]\n",
      "loss: 0.000230  Batch: [  500/ 1411]\n",
      "loss: 0.000002  Batch: [  600/ 1411]\n",
      "loss: 0.000000  Batch: [  700/ 1411]\n",
      "loss: 0.000088  Batch: [  800/ 1411]\n",
      "loss: 0.000248  Batch: [  900/ 1411]\n",
      "loss: 0.000110  Batch: [ 1000/ 1411]\n",
      "loss: 0.000229  Batch: [ 1100/ 1411]\n",
      "loss: 0.000000  Batch: [ 1200/ 1411]\n",
      "loss: 0.000068  Batch: [ 1300/ 1411]\n",
      "loss: 0.000054  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.104393 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.015853  Batch: [    0/ 1411]\n",
      "loss: 0.000003  Batch: [  100/ 1411]\n",
      "loss: 0.000113  Batch: [  200/ 1411]\n",
      "loss: 0.005742  Batch: [  300/ 1411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.000354  Batch: [  400/ 1411]\n",
      "loss: 0.000210  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000001  Batch: [  700/ 1411]\n",
      "loss: 0.000080  Batch: [  800/ 1411]\n",
      "loss: 0.000276  Batch: [  900/ 1411]\n",
      "loss: 0.000118  Batch: [ 1000/ 1411]\n",
      "loss: 0.000207  Batch: [ 1100/ 1411]\n",
      "loss: 0.000000  Batch: [ 1200/ 1411]\n",
      "loss: 0.000059  Batch: [ 1300/ 1411]\n",
      "loss: 0.000026  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.094045 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.013770  Batch: [    0/ 1411]\n",
      "loss: 0.000005  Batch: [  100/ 1411]\n",
      "loss: 0.000101  Batch: [  200/ 1411]\n",
      "loss: 0.005400  Batch: [  300/ 1411]\n",
      "loss: 0.000326  Batch: [  400/ 1411]\n",
      "loss: 0.000192  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000003  Batch: [  700/ 1411]\n",
      "loss: 0.000070  Batch: [  800/ 1411]\n",
      "loss: 0.000299  Batch: [  900/ 1411]\n",
      "loss: 0.000124  Batch: [ 1000/ 1411]\n",
      "loss: 0.000186  Batch: [ 1100/ 1411]\n",
      "loss: 0.000002  Batch: [ 1200/ 1411]\n",
      "loss: 0.000053  Batch: [ 1300/ 1411]\n",
      "loss: 0.000012  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.084707 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.011950  Batch: [    0/ 1411]\n",
      "loss: 0.000006  Batch: [  100/ 1411]\n",
      "loss: 0.000090  Batch: [  200/ 1411]\n",
      "loss: 0.005085  Batch: [  300/ 1411]\n",
      "loss: 0.000304  Batch: [  400/ 1411]\n",
      "loss: 0.000176  Batch: [  500/ 1411]\n",
      "loss: 0.000001  Batch: [  600/ 1411]\n",
      "loss: 0.000008  Batch: [  700/ 1411]\n",
      "loss: 0.000060  Batch: [  800/ 1411]\n",
      "loss: 0.000316  Batch: [  900/ 1411]\n",
      "loss: 0.000128  Batch: [ 1000/ 1411]\n",
      "loss: 0.000168  Batch: [ 1100/ 1411]\n",
      "loss: 0.000004  Batch: [ 1200/ 1411]\n",
      "loss: 0.000047  Batch: [ 1300/ 1411]\n",
      "loss: 0.000004  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.076465 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.010369  Batch: [    0/ 1411]\n",
      "loss: 0.000008  Batch: [  100/ 1411]\n",
      "loss: 0.000080  Batch: [  200/ 1411]\n",
      "loss: 0.004797  Batch: [  300/ 1411]\n",
      "loss: 0.000285  Batch: [  400/ 1411]\n",
      "loss: 0.000162  Batch: [  500/ 1411]\n",
      "loss: 0.000000  Batch: [  600/ 1411]\n",
      "loss: 0.000014  Batch: [  700/ 1411]\n",
      "loss: 0.000052  Batch: [  800/ 1411]\n",
      "loss: 0.000318  Batch: [  900/ 1411]\n",
      "loss: 0.000131  Batch: [ 1000/ 1411]\n",
      "loss: 0.000150  Batch: [ 1100/ 1411]\n",
      "loss: 0.000007  Batch: [ 1200/ 1411]\n",
      "loss: 0.000042  Batch: [ 1300/ 1411]\n",
      "loss: 0.000000  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.069178 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.009090  Batch: [    0/ 1411]\n",
      "loss: 0.000009  Batch: [  100/ 1411]\n",
      "loss: 0.000071  Batch: [  200/ 1411]\n",
      "loss: 0.004535  Batch: [  300/ 1411]\n",
      "loss: 0.000266  Batch: [  400/ 1411]\n",
      "loss: 0.000150  Batch: [  500/ 1411]\n",
      "loss: 0.000000  Batch: [  600/ 1411]\n",
      "loss: 0.000019  Batch: [  700/ 1411]\n",
      "loss: 0.000045  Batch: [  800/ 1411]\n",
      "loss: 0.000313  Batch: [  900/ 1411]\n",
      "loss: 0.000132  Batch: [ 1000/ 1411]\n",
      "loss: 0.000130  Batch: [ 1100/ 1411]\n",
      "loss: 0.000011  Batch: [ 1200/ 1411]\n",
      "loss: 0.000038  Batch: [ 1300/ 1411]\n",
      "loss: 0.000001  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.062717 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.007975  Batch: [    0/ 1411]\n",
      "loss: 0.000010  Batch: [  100/ 1411]\n",
      "loss: 0.000059  Batch: [  200/ 1411]\n",
      "loss: 0.004293  Batch: [  300/ 1411]\n",
      "loss: 0.000252  Batch: [  400/ 1411]\n",
      "loss: 0.000132  Batch: [  500/ 1411]\n",
      "loss: 0.000000  Batch: [  600/ 1411]\n",
      "loss: 0.000024  Batch: [  700/ 1411]\n",
      "loss: 0.000040  Batch: [  800/ 1411]\n",
      "loss: 0.000312  Batch: [  900/ 1411]\n",
      "loss: 0.000131  Batch: [ 1000/ 1411]\n",
      "loss: 0.000116  Batch: [ 1100/ 1411]\n",
      "loss: 0.000015  Batch: [ 1200/ 1411]\n",
      "loss: 0.000034  Batch: [ 1300/ 1411]\n",
      "loss: 0.000004  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.057037 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.006968  Batch: [    0/ 1411]\n",
      "loss: 0.000011  Batch: [  100/ 1411]\n",
      "loss: 0.000049  Batch: [  200/ 1411]\n",
      "loss: 0.004071  Batch: [  300/ 1411]\n",
      "loss: 0.000248  Batch: [  400/ 1411]\n",
      "loss: 0.000122  Batch: [  500/ 1411]\n",
      "loss: 0.000000  Batch: [  600/ 1411]\n",
      "loss: 0.000029  Batch: [  700/ 1411]\n",
      "loss: 0.000035  Batch: [  800/ 1411]\n",
      "loss: 0.000309  Batch: [  900/ 1411]\n",
      "loss: 0.000124  Batch: [ 1000/ 1411]\n",
      "loss: 0.000114  Batch: [ 1100/ 1411]\n",
      "loss: 0.000019  Batch: [ 1200/ 1411]\n",
      "loss: 0.000032  Batch: [ 1300/ 1411]\n",
      "loss: 0.000010  Batch: [ 1400/ 1411]\n",
      "Test Error: \n",
      " Avg loss: 0.052072 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train(train_dataset, model, loss_fn, optimizer)\n",
    "    test(test_dataset, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46030f1b",
   "metadata": {},
   "source": [
    "#### Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cee7f2be-5c4e-40c9-8a35-f503011f1726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next-day price based on 50 previous ones: 28399.970703125\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "data = dataset['Close']\n",
    "data = torch.Tensor(data[-50:].values)\n",
    "output = model(data)\n",
    "print(f'Predicted next-day price based on 50 previous ones: {output.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc1240",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca635949",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot got an unexpected keyword argument 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DD7D~1\\AppData\\Local\\Temp/ipykernel_17680/2300314387.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m plt.plot(x= 'EPOCHS', y=\"trainlosses\",\n\u001b[0m\u001b[0;32m      2\u001b[0m  color=\"green\", alpha=0.8, legend=\"Train loss\", line_width=2)\n\u001b[0;32m      3\u001b[0m plt.plot(x= \"EPOCHS\", y=\"vallosses\",\n\u001b[0;32m      4\u001b[0m  color=\"red\", alpha=0.8, legend=\"Val loss\", line_width=2)\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2755\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2756\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2757\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   2758\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2759\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1630\u001b[0m         \"\"\"\n\u001b[0;32m   1631\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1632\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1633\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpos_only\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m\"xy\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpos_only\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m                 raise TypeError(\"{} got an unexpected keyword argument {!r}\"\n\u001b[0m\u001b[0;32m    248\u001b[0m                                 .format(self.command, pos_only))\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: plot got an unexpected keyword argument 'x'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(x= 'EPOCHS', y=\"trainlosses\",\n",
    " color=\"green\", alpha=0.8, legend=\"Train loss\", line_width=2)\n",
    "plt.plot(x= \"EPOCHS\", y=\"vallosses\",\n",
    " color=\"red\", alpha=0.8, legend=\"Val loss\", line_width=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d481145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
